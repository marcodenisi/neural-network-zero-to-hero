{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb662c0-14c5-4cb9-a06a-399cd9325bff",
   "metadata": {},
   "source": [
    "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e9a05d-45c0-4565-82bc-5059c4c421ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../../data/names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e499dd-f732-4bb7-b256-cfcde2a7ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6114a-971b-49a7-af32-beed8fff9865",
   "metadata": {},
   "source": [
    "Create string to int and int to string dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42bb38ae-76f7-42d2-8b95-1ecccbc5e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7730239-5eb6-4413-9b58-a381975a4174",
   "metadata": {},
   "source": [
    "Populating a 3 dimensional matrix to count trigram occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac6e891-19ad-4190-9a20-adbf332e205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros(27, 27, 27)\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] # add special start and end chars\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4302a37-2c5d-4ac7-b301-6c666fb3a4ac",
   "metadata": {},
   "source": [
    "Normalizing the matrix to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df678270-ec58-45f3-b694-f8944edbd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float() # model smoothing: we add one to the N matrix to avoid having trigrams with 0 occurrences (that is, probability = 0)\n",
    "P /= P.sum(2, keepdim=True) # we want to divide by the sum of the third dimension (0 indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11fb6f-6909-4c56-a74d-fd472911681b",
   "metadata": {},
   "source": [
    "Let's try to generate some names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19bcdf0c-bfd5-4c91-8530-617af1eee6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce.\n",
      "za.\n",
      "zogh.\n",
      "uriana.\n",
      "kaydnevonimittain.\n",
      "luwak.\n",
      "ka.\n",
      "da.\n",
      "samiyah.\n",
      "javer.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        p = P[ix1][ix2]\n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])    \n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        ix1 = ix2\n",
    "        ix2 = ix3\n",
    "        \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e264d63-b988-4ba9-b0c1-478831292998",
   "metadata": {},
   "source": [
    "Now, let's take a step back and calculate the loss function (negative log likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f736c16-805a-40d7-955e-ab09ac2441fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-410414.9688)\n",
      "nll=tensor(410414.9688)\n",
      "2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] # add special start and end chars\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}') # this is our loss function! the lower the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54c0f7-4a4b-4a1c-82ae-a225db98531c",
   "metadata": {},
   "source": [
    "Let's now try to do the same thingy with a neural network. The first thing that comes to mind is that a tensor needs to be an integer: I should map every single 2 chars word to a single int. that would be 27*27 right?\n",
    "\n",
    "What about instead using a 2d tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cbb76f50-dc23-43ea-b57f-6bf56a6edb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training set\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.'] # add special start and end chars\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "abc16114-11ad-4cc0-aeeb-c4033b22c6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        ...,\n",
       "        [26, 25],\n",
       "        [25, 26],\n",
       "        [26, 24]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4dfc152a-1760-4e9f-997e-3a4d3fcc1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27*2, 27), requires_grad=True) # create a tensor filled with random numbers from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c7628796-028e-4935-a79f-b5a98ee208e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2, 27])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9a951a89-3eaf-448b-a649-8855868ee679",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc_view = xenc.view(-1, 27*2) # resize the xenc tensor to be compatible with the multiplication by W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9fa3eb95-c3d4-44f6-b8f9-afdea026f365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 54])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "42acbb32-80af-4ce0-8082-0c9fee3e01e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 27])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9dcca50f-c2cb-499e-a8a4-77622030a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2441227436065674\n",
      "2.244115114212036\n",
      "2.244107723236084\n",
      "2.244100570678711\n",
      "2.2440929412841797\n",
      "2.2440857887268066\n",
      "2.2440783977508545\n",
      "2.2440712451934814\n",
      "2.2440640926361084\n",
      "2.244056463241577\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    # forward pass\n",
    "    logits = xenc_view @ W # log-counts\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set grad to 0\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -10 * W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
